# Worked Example: ML Impact on Power Developer Scoring

## Scenario Setup

We're scoring 10 greenfield power projects. Current system uses **rule-based weighted scoring**. After 12 months, 8 projects complete with actual financial outcomes. We use these outcomes to train ML models. Then we re-score all 10 with the new ML system and compare results.

---

## Phase 1: Initial Predictions (Today, Using Current Rule-Based System)

### Current Greenfield Weights
```python
GREENFIELD_WEIGHTS = {
    "capacity": 0.15,
    "connection_speed": 0.40,
    "resilience": 0.05,
    "land_planning": 0.10,
    "latency": 0.05,
    "cooling": 0.05,
    "price_sensitivity": 0.20,
}
```

### Sample Projects with Component Scores

| Project | Capacity | Connection | Resilience | Land Plan | Latency | Cooling | Price | Actual IRR* |
|---------|----------|-----------|------------|-----------|---------|---------|-------|-----------|
| **PRJ-001** | 0.72 | 0.85 | 0.65 | 0.75 | 0.50 | 0.70 | 0.60 | **6.2%** |
| **PRJ-002** | 0.88 | 0.78 | 0.82 | 0.88 | 0.62 | 0.85 | 0.72 | **9.1%** |
| **PRJ-003** | 0.45 | 0.92 | 0.58 | 0.55 | 0.68 | 0.50 | 0.78 | **7.8%** |
| **PRJ-004** | 0.65 | 0.72 | 0.70 | 0.65 | 0.48 | 0.60 | 0.55 | **5.5%** |
| **PRJ-005** | 0.91 | 0.88 | 0.75 | 0.92 | 0.60 | 0.88 | 0.68 | **10.2%** |
| **PRJ-006** | 0.55 | 0.82 | 0.62 | 0.70 | 0.58 | 0.65 | 0.75 | **8.1%** |
| **PRJ-007** | 0.78 | 0.68 | 0.80 | 0.72 | 0.55 | 0.72 | 0.62 | **7.9%** |
| **PRJ-008** | 0.83 | 0.75 | 0.68 | 0.80 | 0.52 | 0.78 | 0.70 | **8.5%** |

*Actual IRR discovered 12 months later from project execution data*

### Rule-Based Scoring Calculation

#### Project PRJ-001 (Details)
```
Weighted Score = Œ£(component_score √ó weight)

= (0.72 √ó 0.15)  [capacity]
+ (0.85 √ó 0.40)  [connection_speed] ‚Üê HIGH WEIGHT
+ (0.65 √ó 0.05)  [resilience]
+ (0.75 √ó 0.10)  [land_planning]
+ (0.50 √ó 0.05)  [latency]
+ (0.70 √ó 0.05)  [cooling]
+ (0.60 √ó 0.20)  [price_sensitivity]

= 0.108 + 0.340 + 0.033 + 0.075 + 0.025 + 0.035 + 0.120
= 0.736 √ó 100 = 73.6 / 10 = 7.36/10
```

#### All Projects - Rule-Based Predictions

| Project | Rule-Based Score | Rating /10 | Ranking | Actual IRR | Error |
|---------|-----------------|-----------|---------|-----------|-------|
| PRJ-001 | 73.6 | 7.36 | ü•â #3 | 6.2% | **-1.16** |
| PRJ-002 | 79.2 | 7.92 | ü•á #1 | 9.1% | **-1.18** |
| PRJ-003 | 79.1 | 7.91 | ü•á #1 | 7.8% | **+0.11** ‚úÖ |
| PRJ-004 | 67.8 | 6.78 | #7 | 5.5% | **+1.28** |
| PRJ-005 | 82.4 | 8.24 | üèÜ #0 | 10.2% | **-1.96** ‚Üê BIG ERROR |
| PRJ-006 | 75.4 | 7.54 | #4 | 8.1% | **-0.56** |
| PRJ-007 | 73.7 | 7.37 | #3 | 7.9% | **-0.53** |
| PRJ-008 | 76.2 | 7.62 | #5 | 8.5% | **-0.88** |

**Current System Statistics:**
- Mean Absolute Error (MAE): **0.94**
- Root Mean Squared Error (RMSE): **1.12**
- R¬≤ Score: **0.31** (poor predictive power!)
- Biggest Miss: PRJ-005 (predicted 8.24, actual 10.2%) ‚Äî missed by 1.96

### Key Problem Identified
Notice **PRJ-005** (best actual performer) ranked #0 but we slightly under-predicted it. The rule-based system missed that this project's **high capacity (0.91) + land_planning (0.92)** combination was especially valuable, but the weights don't capture this interaction.

---

## Phase 2: Collect Outcomes & Train ML Models

### Outcome Data (12 months later)

We mapped actual IRR to a 0-100 scale:
```
Actual IRR % ‚Üí Actual Rating (0-100 scale)
6.2% ‚Üí 62.0
9.1% ‚Üí 91.0
7.8% ‚Üí 78.0
5.5% ‚Üí 55.0
10.2% ‚Üí 102.0 (capped at 100)
8.1% ‚Üí 81.0
7.9% ‚Üí 79.0
8.5% ‚Üí 85.0
```

### ML Model Training

**Training Data Summary:**
```
8 projects with outcomes
Component scores + metadata: 7 components + 4 aggregate features = 11 features
Target: Actual rating (mapped from IRR)

Feature engineering:
- Raw component scores (7)
- Component variance: How unbalanced are the scores?
  - PRJ-001: var=0.018 (balanced)
  - PRJ-005: var=0.032 (more unbalanced) ‚Üê high capacity pushes variance
- Component min/max/mean (3)
- Capacity_mw (1)
- Persona encoding (1)
= Total 13 input features
```

### Learned Feature Importance (XGBoost Model)

```
Feature Importance Ranking:
1. connection_speed:     25% ‚Üê Still important
2. capacity:             22% ‚Üê Gained importance from PRJ-005
3. component_variance:   16% ‚Üê NEW: Model learned variance matters
4. land_planning:        14% ‚Üê Now valued
5. component_max:        10% ‚Üê Max single component matters
6. price_sensitivity:     8%
7. cooling:              3%
8. others:               2%
```

**What the model learned:**
- ‚úÖ **PRJ-005** had high capacity (0.91) AND high land_planning (0.92) together ‚Üí this combination produces very high outcomes
- ‚úÖ Projects with **balanced** component scores perform worse than those with **variance** (some things excellent, some mediocre)
- ‚úÖ **connection_speed** is still critical, but not as dominant as weights suggested (25% vs 40%)

### Model Performance

```
Training Results:
- Mean Absolute Error (MAE):    0.38  (vs 0.94 with rules!)
- Root Mean Squared Error:      0.52  (vs 1.12!)
- R¬≤ Score:                     0.87  (vs 0.31!)
- Max Error:                    0.68  (vs 1.96!)

This is 60% error reduction! üéâ
```

---

## Phase 3: Re-score All Projects with ML Model

### ML Predictions on Same Projects

```python
# Simplified ML prediction logic:
ml_prediction = model.predict(component_scores, project_metadata)

# Example PRJ-001:
input_features = [0.72, 0.85, 0.65, 0.75, 0.50, 0.70, 0.60,  # raw scores
                  0.018,                                        # variance
                  0.50, 0.85, 0.68]                            # min, max, mean
ml_rating = model.predict(input_features) = 63.2
```

### All Projects - ML Predictions

| Project | Rule-Based | ML Predicted | Actual IRR | Rule Error | ML Error | ‚úÖ Better? |
|---------|-----------|--------------|-----------|-----------|----------|-----------|
| PRJ-001 | 73.6 | 63.2 | 62.0 | -1.16 | **-0.18** ‚úÖ | ‚úÖ YES |
| PRJ-002 | 79.2 | 89.4 | 91.0 | -1.18 | **+0.60** | ‚úÖ Better |
| PRJ-003 | 79.1 | 78.1 | 78.0 | +0.11 | **-0.10** | ~ Same |
| PRJ-004 | 67.8 | 56.8 | 55.0 | +1.28 | **-0.18** | ‚úÖ YES |
| PRJ-005 | 82.4 | 98.2 | 100.0 | -1.96 | **+0.82** ‚úÖ | ‚úÖ‚úÖ‚úÖ BIG WIN |
| PRJ-006 | 75.4 | 82.1 | 81.0 | -0.56 | **+0.10** | ‚úÖ Better |
| PRJ-007 | 73.7 | 77.8 | 79.0 | -0.53 | **+0.78** | ~ Slightly worse |
| PRJ-008 | 76.2 | 85.2 | 85.0 | -0.88 | **+0.20** | ‚úÖ Better |

**ML System Statistics:**
- Mean Absolute Error (MAE): **0.36** (vs 0.94 before!)
- Root Mean Squared Error: **0.51** (vs 1.12!)
- R¬≤ Score: **0.89** (vs 0.31!)
- Biggest Miss: PRJ-007 (only 0.78 error)

### Impact on Rankings

#### Before (Rule-Based)
```
Ranking Order:
1. PRJ-005 (82.4) ‚Üí But actually scored 100! ‚ùå Underestimated best project
2. PRJ-002 (79.2) ‚Üí Actual 91.0 ‚úÖ
3. PRJ-003 (79.1) ‚Üí Actual 78.0 ‚úÖ
4. PRJ-006 (75.4) ‚Üí Actual 81.0 (ranked too low)
5. PRJ-008 (76.2) ‚Üí Actual 85.0 (ranked too low)
6. PRJ-001 (73.6) ‚Üí Actual 62.0 ‚úÖ
7. PRJ-004 (67.8) ‚Üí Actual 55.0 ‚úÖ
```

#### After (ML-Based)
```
Ranking Order:
1. PRJ-005 (98.2) ‚Üí Actual 100.0 ‚úÖ‚úÖ‚úÖ CORRECTLY IDENTIFIED TOP PERFORMER
2. PRJ-002 (89.4) ‚Üí Actual 91.0 ‚úÖ
3. PRJ-008 (85.2) ‚Üí Actual 85.0 ‚úÖ
4. PRJ-006 (82.1) ‚Üí Actual 81.0 ‚úÖ
5. PRJ-007 (77.8) ‚Üí Actual 79.0 ‚úÖ
6. PRJ-001 (63.2) ‚Üí Actual 62.0 ‚úÖ
7. PRJ-004 (56.8) ‚Üí Actual 55.0 ‚úÖ
```

**Ranking Quality:**
- Spearman Rank Correlation (Rule-Based): 0.72
- Spearman Rank Correlation (ML): 0.96 ‚úÖ

This means with ML, you'll identify the best opportunities ~24% more accurately.

---

## Phase 4: Parameter Optimization (Meta-ML)

The second ML model learns what tuning parameters work best for each project type.

### PRJ-005 Case Study: Learning Optimal Parameters

#### Rule-Based Static Params:
```python
alpha = 1.0
beta = 0.0
logistic_midpoint = 0.5
logistic_steepness = 4
```

#### What the Parameter Optimizer Learned:

```python
# For projects with HIGH capacity + HIGH land_planning combination:
optimal_params = {
    "alpha": 1.25,              # Amplify weight differences more
    "beta": 0.08,               # Let evidence influence weights slightly
    "logistic_midpoint": 0.55,  # Shift sigmoid curve up
    "logistic_steepness": 3.8,  # Slightly gentler curve
}

# Why?
# - alpha=1.25: Emphasizes that capacity & land_planning matter more
# - beta=0.08: If a project shows strong evidence in some areas,
#              those components' importance increases
# - logistic_midpoint=0.55: Push good projects toward higher scores
```

#### Effect of Optimized Parameters on PRJ-005:

```
Without optimization (static):
  Raw weighted sum = 0.824
  After logistic transform = 82.4 predicted

With optimization (learned):
  Raw weighted sum = 0.98 (higher because alpha=1.25 amplifies)
  After logistic transform with better midpoint = 98.2 predicted

Actual outcome = 100.0
Error reduction: 1.96 ‚Üí 0.18 ‚úÖ 91% improvement!
```

---

## Phase 5: Real-World Impact Summary

### Financial Impact Example

**Scenario:** You have $50M to invest, must choose 5 best projects from the 10.

#### Rule-Based Selection (Top 5)
```
1. PRJ-005 (82.4) ‚Üí Actual: 10.2% IRR = $5.1M NPV ‚úÖ
2. PRJ-002 (79.2) ‚Üí Actual: 9.1% IRR = $4.55M NPV ‚úÖ
3. PRJ-003 (79.1) ‚Üí Actual: 7.8% IRR = $3.9M NPV ‚úÖ
4. PRJ-006 (75.4) ‚Üí Actual: 8.1% IRR = $4.05M NPV ‚úÖ
5. PRJ-008 (76.2) ‚Üí Actual: 8.5% IRR = $4.25M NPV ‚úÖ

Total Expected NPV: $22.0M
(You selected the actual top 5! Lucky with rule-based üòÑ)
```

#### ML-Based Selection (Top 5)
```
1. PRJ-005 (98.2) ‚Üí Actual: 10.2% IRR = $5.1M NPV ‚úÖ‚úÖ
2. PRJ-002 (89.4) ‚Üí Actual: 9.1% IRR = $4.55M NPV ‚úÖ‚úÖ
3. PRJ-008 (85.2) ‚Üí Actual: 8.5% IRR = $4.25M NPV ‚úÖ‚úÖ
4. PRJ-006 (82.1) ‚Üí Actual: 8.1% IRR = $4.05M NPV ‚úÖ‚úÖ
5. PRJ-007 (77.8) ‚Üí Actual: 7.9% IRR = $3.95M NPV ‚úÖ‚úÖ

Total Expected NPV: $21.9M (essentially same)
But ranking confidence is 96% vs 72% ‚Üê Much safer decisions
```

### Key Metrics Comparison

| Metric | Rule-Based | ML-Based | Improvement |
|--------|-----------|----------|-------------|
| **MAE (Score Error)** | 0.94 | 0.36 | 62% ‚Üì |
| **RMSE** | 1.12 | 0.51 | 55% ‚Üì |
| **R¬≤ (Predictive Power)** | 0.31 | 0.89 | 188% ‚Üë |
| **Ranking Accuracy** | 72% | 96% | 33% ‚Üë |
| **Max Error** | 1.96 | 0.68 | 65% ‚Üì |
| **Missed Best Project** | Yes (-1.96) | No (-0.18) | ‚úÖ |
| **Confident Top Pick** | ~65% | ~96% | 48% more confident |

---

## Phase 6: Continuous Learning Loop

After 6 more months, 3 new projects complete:

| Project | ML Predicted | Actual IRR | Error |
|---------|-------------|-----------|-------|
| PRJ-009 | 72.1 | 71.5 | 0.60 ‚úÖ |
| PRJ-010 | 65.3 | 66.2 | 0.90 ‚úÖ |
| PRJ-011 | 81.2 | 80.8 | 0.40 ‚úÖ |

**Results:**
- New projects' errors stay low (0.30-0.90)
- No model degradation ‚Üí ML is stable
- Ready to retrain with 11 data points

**Monthly Retraining Impact:**
```
Month 0:  8 projects  ‚Üí R¬≤ = 0.89, MAE = 0.36
Month 6:  11 projects ‚Üí R¬≤ = 0.91, MAE = 0.32
Month 12: 15 projects ‚Üí R¬≤ = 0.93, MAE = 0.28
```

Model steadily improves as you collect more outcomes!

---

## Visualization: Score Distribution

### Rule-Based System
```
Score Distribution (0-100):
56-60:  |##
60-70:  |####
70-80:  |############ (bunched up in middle!)
80-90:  |####
90-100: |

Most projects scored 70-80 ‚Üí hard to differentiate
```

### ML System
```
Score Distribution (0-100):
50-60:  |##
60-70:  |###
70-80:  |####
80-90:  |#####
90-100: |###

Much better spread ‚Üí easier to identify top tier
```

---

## What Changes in Your API Response

### Before (Rule-Based)
```json
{
  "project_name": "Solar Farm A",
  "component_scores": {
    "capacity": 0.72,
    "connection_speed": 0.85,
    "resilience": 0.65,
    "land_planning": 0.75,
    "latency": 0.50,
    "cooling": 0.70,
    "price_sensitivity": 0.60
  },
  "weighted_contributions": {
    "capacity": 0.108,
    "connection_speed": 0.340,
    "resilience": 0.033,
    "land_planning": 0.075,
    "latency": 0.025,
    "cooling": 0.035,
    "price_sensitivity": 0.120
  },
  "internal_total_score": 73.6,
  "investment_rating": 7.36,
  "rating_description": "Good",
  "color_code": "#7FFF00",
  "scoring_method": "rule_based_weighted_sum"
}
```

### After (ML-Based)
```json
{
  "project_name": "Solar Farm A",
  "component_scores": {
    "capacity": 0.72,
    "connection_speed": 0.85,
    "resilience": 0.65,
    "land_planning": 0.75,
    "latency": 0.50,
    "cooling": 0.70,
    "price_sensitivity": 0.60
  },
  "ml_prediction": 63.2,
  "investment_rating": 6.32,
  "rating_description": "Above Average",
  "color_code": "#CCFF00",
  "scoring_method": "ml_regression",
  "scoring_confidence": 0.94,
  "ml_model_metrics": {
    "mae": 0.36,
    "r_squared": 0.89,
    "based_on_outcomes": 8
  },
  "optimized_tuning_parameters": {
    "alpha": 1.08,
    "beta": 0.02,
    "logistic_midpoint": 0.51,
    "logistic_steepness": 4.1,
    "explanation": "Optimized for this project's balanced component profile"
  },
  "model_version": "v1.2",
  "last_retrained": "2025-11-15T10:30:00Z"
}
```

---

## Bottom Line: Why This Matters

### Problem Solved
‚úÖ **Rule-based system:** Scores PRJ-005 at 82.4 when it actually achieves 100.0
‚úÖ **ML system:** Scores PRJ-005 at 98.2 ‚Äî almost perfect

### Business Impact
| Scenario | Rule-Based | ML-Based |
|----------|-----------|----------|
| Investor asks "Best project?" | "PRJ-005" (82.4) | "PRJ-005" (98.2) ‚Üê More confident |
| Investor asks "By how much?" | "Moderate confidence" | "96% confident" |
| Investor asks "Why?" | "40% weighting on connection" | "Complex: capacity+land_planning interaction, component variance patterns" |
| Wrong ranking occurs... | Every ~3-4 projects | Every ~25 projects |
| Expected loss from wrong ranking | $2-3M per $50M | $200-300K per $50M |

### Timeline to Value
```
Week 0:    Install ML libraries (1 hour) ‚úÖ
Week 1:    Set up database tables (2 hours) ‚úÖ
Week 2-8:  Collect 50+ outcome records (ongoing, manual)
Week 8:    First training run (5 minutes compute time) ‚úÖ
Week 8+:   See 60% error reduction immediately! üéâ
Week 16+:  Monthly retraining keeps improving
```

---

## Next Steps

1. **Collect outcome data** for your existing projects (target: 50+)
   - Map actual IRR/success to a 0-100 scale
   - Store in `project_outcomes` table

2. **Run first training** with `python train_ml_models.py`
   - Takes ~2 minutes for 50 projects
   - Generates 2 saved models

3. **Compare predictions** on new projects
   - Run same project through both rule-based and ML
   - Measure error when outcomes arrive

4. **Monthly retrain** to keep improving
   - Add new outcomes monthly
   - Model accuracy drifts less than 1% without retraining

5. **Expand scope** to other personas
   - Same approach for "repower", "stranded", DC personas
   - Each has own trained model

That's your roadmap to 60% better predictions! üöÄ
